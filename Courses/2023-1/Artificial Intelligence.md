# Artificial Intelligence

## Profesor: V√≠ctor Germ√°n Mijangos de la Cruz

El campo de la Inteligencia Artificial (IA) se enfoca tanto en el estudio como en la
construcci√≥n de entidades inteligentes. 

Podemos plantearnos las siguientes preguntas:
- ¬øQu√© tipo de entidades nos interesan?
- ¬øQu√© implica que una entidad sea inteligente?

Podemos distinguir los siguientes √°mbitos:
- Considerando si se refiere al humano:
    1. Inteligencia en tanto fidelidad al actuar humano: 
        - Pensamiento del humano: Podemos decir que una entidad es inteligente si sabemos su programaci√≥n es similar a la forma en que pensamos los humanos. Las ciencias cognitivas buscan construir teor√≠as del cerebro humano.
        
        Algunos pasos que se pueden seguir son:
        - IntrospeccYa quei√≥n
        - Experimentaci√≥n psicol√≥gica
        - Im√°genes cerebrales

    2. Inteligencia en tanto ‚Äúllegar a un buen fin‚Äù o racionalidad:
        - Pensamiento racional: Una entidad es inteligente si su pensamiento es el adecuado para llegar a la soluci√≥n de un problema. Necesitamos de un pensamiento l√≥gico que nos lleve a tomar las decisiones adecuadas.

- Considerando el punto de referencia:
    1. Inteligencia como atributo interno, como procesos de pensamiento y razonamiento.
    2. Inteligencia como atributo externo, comportamiento.

## Day 2
### Enfoques:

        -  | Interno | Exterior|
____ 
    Humano | Cognici√≥n | Racional|
___
    Racional| 
___
* Comportamiento Racional:
    - Agente: Algo que actua, a partir de su ambiente percibido y adaptandose.
        - Agente racional: Actuar de una forma √≥ptima para obtener el mejor resultado esperado.
* Modelo Est√°ndar: Agentes racionales que solucionen tareas de manera adecuada,
    - Las inferencias correctas son parte de sus mecanismos.
    - Adaptable a perspectivas cient√≠ficas, usa formalismos matem√°ticos para representarlos.

* History of the AI:
    - Maquinas pensantes: Resolver problemas a partir de la l√≥gica. _La inteligencia se asocia al pensamiento l√≥gico_.

    - Tesis de Turing:
        > Para culquier sistema forma determinista, existe una MT formalmente equivalente.
    - **Bar-Hillel**:
        > Cualquier sistema capaz de hacer una traducci√≥n aut√©ntica y precisa debe saber lo que sabe la gente y debe aplicar ese conocimiento de forma sensata. 
    - M√©todos d√©biles: mecanismos con pr√≥posito general que usan pasos de razonamiento elemental para encontrar soluciones completas.[dominio espec√≠fico]

    - Micromundos: Donde interactua nuestro agente, un entorno delimitado[dominio espec√≠fico].
    - Sistemas expertos: limitar la actuaci√≥n de la m√°quina a un dominio espec√≠fico buscando que se comporte como un humano experto.
    - En los √∫ltimos a√±os todo est√° en redes neuronales[Deep Learning]

## Monday August 22

* Agente: Entidad que tiene sensores que le ayudan a percibir el ambiente a traves de sensores
    
    - Secuencia de percepciones = historia completa de lo percibido por el agente hasta el momento actual.

    - Una **acci√≥n** depende del conocimiento que tenemos a partir de su secuencia de percepciones, *actuar* a partir de percepciones pasadas[experiencia].
        - Entre m√°s experiencia podremos tomar una mejor decisi√≥n

    - **Funci√≥n de agente:** Mapeo utilizado para pasar de la cadena de percepciones hacia las acciones. La funci√≥n mapea las percepciones de las acciones:
        - input: secuencia de persepciones
        - output: acci√≥n
    Esta determinara el comportamiento de nuestro agente. (A√∫n tenemos limitaciones, por el determinismo)

* Racionalidad y agentes racionales:
    - Agente que realiza acciones √≥ptimas con base en su experiencia.
        + √ìptimo: El consecualismo se√±ala que podemos evaluar el comportamiento de un agente a partir de sus consecuencias necesitando as√≠ poder medir el rendimiento:
            * Es decir, necesitamos medir nuestro rendimiento[es decir, nuestras consecuencias]
                - Regla general: es mejor medir la utilidad de acuerdo con lo que se quiere en el entorno, m√°s de acuerdo con c√≥mo se cree que el agente debe comportarse.
                - Medida de rendimiento: 

    - Racionalidad: 
        1. Medida de rendimiento que define el criterio de √©xito.
        2. Conocimiento previo acerca del ambiente[entorno].
        3. Acciones que se pueden realizar
        4. Secuencia de percepciones[experiencia, memoria]
    
    Buscando una mejora con el tiempo. 
    
    - Agente racional: Agente que selecciona la _acci√≥n_ que maximice la _medida de rendimiento_ a partir de la _experiencia_(secuencia de percepciones) y su _conocimiento previo_ a cerca del ambiente.

    - Agente omniciente: Agente que conoce  el resultado de sus acciones y puede actuar de acuerdo a esto.

    - Tipos de agente:
        1. Agente activo: Todo depende del estado actual, ya que es lo √∫nico que ve: _el entorno actual_ y a partir de ah√≠ decide como act√∫a.
        2. Adente dividido por tabla: Es capaz de usar memoria

    - Racionalidad busca maximizar el rendimiento **esperado**
    - La perfecci√≥n maximiza el redimiento **real**.
        + Racionalidad != Omniciencia[Dios]

## Assitant Tuesday August 23

- IA: Simular el comportamiento humano. Acciones a√∫tonomas aunque no aprenda.
- ML: La maquina aprende, a traves de la experiencia.
- DL: Aprendizaje utilizando redes neuronales.

Un agente dentro de un determinado entorno necesita de sensores para percibir las persepciones de su entorno y de un actuador[capacidad del agente para definir x acci√≥n] que nos permite hacer acciones en nuestro entorno[modificandolo], el agente tiene una metrica de evaluaci√≥n y a partir de esa evaluaci√≥n tenemos una respuesta[acci√≥n] (funci√≥n de transici√≥n)

## Class Wednesday August 24

* Naturaleza de los ambientes:
    - Entorno de trabajo: es especificado por la medida de rendimiento, el ambiente, as√≠ como los actuadores y los sensores del agente.
    
    - En el dise√±o del agente implica dise√±ar los elementos del entorno de trabajo:
        + Medidas de rendimiento
        + Entorno
        + Actuadores
        + Sensores
    
    - Entornos observables:
        + Entorno completamente observable:
            * El entorno es as√≠ si los sensores del agente tiene acceso a la informaci√≥n total de un estado del ambiente en x tiempo.
            * Toda la informaci√≥n **de todo el entorno** en un estado del ambiente
        + Entorno parcialmente observable: 
            + informaci√≥n parcial del ambiente
                * Manejo automatico de autos
        + Efectivamente totalmente observable:
            - **Todo desde la perspectiva del agente-**
            - Aparte de toda la info, tenemos la suficiente informaci√≥n para saber que acci√≥n realizar **la adecuada**.
            -  Si el agente detecta los aspectos relevantes para realizar una acci√≥n.
            (Si supieramos como se mueven los fantasmas podr√≠amos decir que siempre nos movemos efectivamente.)
        + No obsevable: Si el agente no tiene sensores, el entorno es no observable.
    
    - El entorno determina si hablamos de un agente individual o multiagentes, depende de las acciones que quieramos realizar.
        + Individual: Un solo agente.
        + Multiagente: Dos o m√°s agentes que se afecten entre s√≠.
            1. Competitivo: Maximizar el rendimiento de un agente implica que se minimiza el rendimiento de otros agentes.
                - Ajedrez
            2. Cooperativo: Las acciones de cada agente se orientan a maximizar el rendimiento de todos los agentes
                - Auto sin conductor
    - Entorno Determinista:
        - El estado est√° determinado completamente por las acciones del agente y de su estado previo(Si existen aspectos no observables, entonces es preferible tratar el entorno como estoc√°stico)
        - Entorno Estoc√°stico(No-Determinista):
            // Una acci√≥n puede llevar a m√°s de un resultado
            - El estado puede cambiar no necesariamente por una acci√≥n del agente.
        - Entorno Estrat√©gico:
            - Las acciones son deterministas pero siempre habr√°n resultados de otros agentes. Hay acciones de otros agentes que pueden modificar el edo del entorno.
            El entorno puede cambiar a pesar de mi si hay otros agentes.
    - Secuencia:
        - Entorno Episodico:
            - Episodios independecias entre si, entre que va a pasar en un episodio y otro, en cada uno de los cuales el agente reciba una persepci√≥n y ejecuta una acci√≥n en base en √©sta.
                * Las acciones son independientes entre si. No hay correlaci√≥n entre s√≠.
        - Entorno secuenciales:
            - La desici√≥n actual puede afectar a las futuras decisiones del agente, acciones influyentes entre si.
    - Dinamica **=!** 
        - Entorno est√°tico: No cambia cuando el agente delibera
        - Entorno din√°mico: Cambia independientemente del agente
        - Semidin√°mico: Cambia nuestra capacidad de reaccionar del agente y cambia el entorno
    - Discreto: estados, percepciones, y acciones finitas 
    - Continuo: Entorno cambiante, cuando es dificil definir estados entre acciones, cuando no es posible enumerar los estados no se si est√° bi
    [Es continuo porque siempre tenemos una desici√≥n diferente.]
        - Conocido: resultados de las acciones conocidos de antemano. [Se sabe el accionar],
        - Desconocido: No hay certeza de como se comporta el entorno, no se conocen los resultados de las acciones, cuando el agente no sabe como va a acabar.
            + Detecci√≥n de spam.
    - En el modelo de lo que quieres, todo es a su conveniencia
    

## Class Monday 29 August

* El problema de un agente dirigido mediante tabla es el n√∫mero de b√∫squedas que debe hacer en la tabla; Entorno: Completamente observable, determinista, est√°tico, discreto. Tiene memor√≠a peroe s

* _Agente reactivo simple_. Ignora la historia de percepciones, los agentes contienen reglas de _condici√≥n-acci√≥n_. Entorno episodico y no guarda nada, ninguna percepci√≥n, solo devuelve el estado

* _Agente reactivo basado en modelos._ es un agente que se basa en un modelo del mundo determinado por nosotros
    - Modelo de transici√≥n, actuadores: modelar las acciones/decisiones
    - Modelo de sensor: Como percibe el agente al mundo, a partir de los sensores vemos como es el mundo en ese momento.

* _Agente basado en objetivo_: Cuenta con informaci√≥n acerca de un objetivo. 

* _Agente basado en utilidad_: indica c√∫ales son preferibles al comparar los resultados, busca maximizar el valor esperado de esta funci√≥n.

* Agente por aprendizaje: _Que aprende_: Se aprende cuando una agente que opera sobre un entorno desconocido mejora su rendimiento a partir de observar resultados previos de sus acciones y evaluarlos.

## Class Wednesday 31 August

- Utilitarismo

- Agentes para resolver problemas:
    + Agente basado en objetivo que realiza una planeaci√≥n para cumplir con los objetivos dados.
    + Para solucionar los problemas el agente se basan en la _b√∫squeda_.
        1. Busqueda Informada: El agente puede estimar que tan lejos se encuentra del objetivo.
        2. BUsqueda Desinformada: No es capaz de estimar su distancia del objetivo, no tenemos informaci√≥n precisa relacionada con el objetivo.
            
            > Si estamos fallando en planear, estamos planeando fallar.

- Para resolver el problema necesitamos definir(la planeaci√≥n):
    - Formulaci√≥n del objetivo: Determinar estado objetivos.
    - Formulaci√≥n del problema: Descripci√≥n de estados y las acciones para alcanzar un objetivo(abstracci√≥n del mundo).
    - B√∫squeda: Simula la secuencia de acciones que lleven al objetivo, elegiendo la m√°s √≥ptima de acuerdo a una funci√≥n que decidamos.
    - Ejecuci√≥n: 

- About example:
    * Las acciones mueven nuestro agente entre los estados.
        + Y las acciones del agente por edo tambi√©n? 
- Sistema de ciclos:
    1. Sistema de ciclos abiertos(Open loop): Una vez que el agente ha encotrado la soluci√≥n, la ejecuta ignorando las persepciones mientras lo hace.[No deja de percibir, solo las ignora.].
    2. Sistema de ciclos cerrado(Closed loop): Son aquellas en el que el agente ejecuta la soluci√≥n, pero mientras lo hace moritorea las percepciones que recibe. [Podemos incorporarlas].

- Problema de b√∫squeda formalizado: 6-tupla = A√∫tomata.
    + S=s1...sn, conjunto de estados
    + S_o in S, edo inicial
    + F subconjunto S, edos finales
    + A = a1...an, conjunto de acciones
    + T: SxA -> S, modelo de transici√≥n.
        - si estoy en s1 con acci√≥n limpiar ent llegamos a s2
    + c: funci√≥n de costo

- Soluciones:
    + Camino: Secuencia de acciones.
    + Soluci√≥n: Camino que te lleva a la soluci√≥n, i.e. termina en un objetivo.
    + Soluci√≥n √≥ptima: Soluci√≥n que minimiza la funci√≥n de costo sobre el camino. 

### Class Thursday September 1

* _Agente reactivo basado en modelos._ es un agente que se basa en un modelo del mundo determinado por nosotros
    - Modelo de transici√≥n, actuadores: modelar las acciones/decisiones
    - Modelo de sensor: Como percibe el agente al mundo, a partir de los sensores vemos como es el mundo en ese momento.

El entorno va cambiando el entorno ddl agente.

* Agente basado en objetivos: 
Estado --> C√≥mo es el mundo ahora? **-->** Estado
Dentro de sus actuadores puede cambiar el estado 

### Class Friday September 2
* No buscamos el m√≠nimo, si no el argumento que m√≠nimiza la funci√≥n. 
    - arg min  $x^2 + 1=1

* Algoritmo de b√∫squeda:
    - Problema de b√∫squeda -> Alg de b√∫squeda -> Soluci√≥n

* √Årbol de b√∫squeda: describe los caminos entre un estado inicial y el objetivo.
    - Edo inicial = raiz
    - Edo final = hoja
    - Expanci√≥n de Nodo: teniendo un nodo se dice expansi√≥n del nodo si podemos cambiar de estado a travez de la funci√≥n de transcici√≥n.
    - Frontera: estados dados que no se ha expandido.
        + Siempre tenemos que revisar la frontera para ver que podemos hacer, ver cual nos conviene seguir expandir, "elegir estrategia"
        + La frontera  es una pila que guarda informaci√≥n de los nodos obtenidos.
            - Pila de prioridad: El tope de la pila es el elem con menor valor de acuerdo a una funci√≥n de costo
            - FIFO(First-in-First Out) EL tope de la cola es el primero
            - LIFO(Last-in-First Out) EL √∫ltimo en entrar en la cola es el primero
            - Operaciones:
                - pop = expandir nuestro nodo
    - Algoritmo Best-First Search, busca encontrar el camino m√°s adecuado.
        - Ocupa hacer pop al estado menos costoso y expandir ese nodo.
        - FIFO pila de prioridad
        - Cuando tenemos los pesos iguales cualquiera es el mejor, por lo que podemos sacar aletoriamente
        - EJEM 37


### Class 07 Sep

* Busqueda desinformada
    - si no contamos con informaci√≥n de la cercan√≠a de este estado con la meta
    - para revisar si un nodo es una meta:
        - Early Goal Test: checar si el nodo es una meta antes de expandir
        - Late Goal Test: checamos el nodo despues de eliminaf de la pila|


### Class 09 Sep
* busqueda desinformada
    EARLY: revisa antes de expandir
    Late: despues

* Breadth-FIst: algoritmo de busca desinformada, expande todos sus hijos, no toma la funci√≥n de costo,frontera: pila FIFO, el primero que entre es el que sale, en un nodo de profundidad d, se han revisado los d-1 niveles.

* Dijkstra: Pila de prioridad dado por la funci√≥n del costo, no podemos usarlo si no tenemos una funci√≥n de costo, porque sino ser√≠a un fuerza bruta

*  Depth-First: No toma en cuenta el costo, expande primero el nodo de mayor prioridad
    - implementacion tipo √°rbol, no guarda una tabla de los nodos alcanzados
    - Best-First Search con funci√≥n negativa(LIFO)
    - devuelve la primera soluci√≥n que encuentra, no la √≥ptima
    - solo es completo cuando tengamos una estructura de √°rbol, en ciclos muere, no podemos garantizar que converga.
    - Ejemplo: 15 A
        - no guarda lo que vamos alcanzando
        - abre todos los hijos de un lado

* Depth-Limited Search: es  un tipo Depth First limitando la profundidad mayor a cierta profundidad
    - (25) fuerza bruta limitada, tope de hasta donde vas a expandir
    - 30 code
    - Problema: no encontrar una soluci√≥n cuando se encuentra m√°s alla de la profundidad escogida.
        - la soluci√≥n est√° en encontrar el diametro, pero eso puede ser imposible calcularlo

* Iterative Deepening Search[40] (mejor versi√≥n de profundidad)
    - Corta en la soluci√≥n exacta, ya que no necesita explorar profundidades de m√°s
    - Completo: porque podra encontrar la soluci√≥n aunque se tarde mucho

### Class 12 Sep
* Bidirectional Search: Buscar de manera simult√°nea en forma de avance(desde el inicio al final) y de forma de retroceso.
    + Ejemplo:25
+ Algoritmos de b√∫squeda desinformada.

+ Busqueda informada (heur√≠stica): se basa en la integraci√≥n de una funci√≥n heur√≠stica:
    - Funci√≥n heur√≠stica: 

    + Greedy Best-First Search:
        - expande en el primer lugar el nodo n con el valor h(n) m√°s peque√±o, es decir: f(n)=h(n).
        - ambicioso por llegar r√°pido sin tomar en cuenta los costos.
        - converge a una soluci√≥n aunque no todos los caminos lleguen a una soluci√≥n
        - Nos basamos en la heuristica
    + Busqueda A* : expande el nodo, n que minimiza la funci√≥n f(n)=g(n)+h(n) donde g es el costo desde el edo init al nodo n, y h es una funci√≥n heur√≠stica al costo del nodo n hacia un edo f.
        - Completo: podemos garantizar que podemos llegar del edo inicial al final sin bucles.
    
### Assistant 13 Sep
algorithm DF: lo m√°s profundo hasta encontrar la solucion
note of A*, plantilla, get_path*
En el 4 las acciones las puedes definir tu [arriba y abajo]

### Class 14 Sep
+ Algoritmo A* 
    - ejemplo: 17
+ Heur√≠stica admisible: es una que nunca sobre-estima el costo de alcanzar una meta:
    - h es admisible si para todo n in V: h(n)<= h* (n). donde h* es el costo √≥ptimo.
    - nos dice exactamente el camino m√°s √≥ptimo.
+ Heur√≠stica consistente: si para todo nodo n y todo sucesor n' generado por la acci√≥n a se tiene que:
    - Nos asegura que cada vez que se alcance un nodo ser√° por un camino √≥ptimo
    - La consistente es un subconjunto de la admisible

+ Mon√≥tona:
    - h se dice que mon√≥tona si h(n)<=h'(n)
        - consistencia implica mon√≥tomia
+ Nodos ciertamente expandidos
+ Dominio: entre heur√≠sticas
    - h1 domina a h2 si para todo nodo se cumple: 
    - Si h1 domina a h2 ent h1 es m√°s eficiente (h2 expande al menos tantos nodos como h1)

### Class 19 Sep
* Factor de ramificaci√≥n eficaz
    - cuantas veces lo elevamos para que nos de el n√∫mero de nodos.
* Algoritmo A* fuerte
    El 20 vimos ejemplo de iteracion del ejercicio 6 de la tarea

### Class 21 Sep
* A* pesado: 
* Beam Search
    - En la frontera siempre tenemos k nodos, los expandimos y nos quedamos con los k mejores
    - Se expanden todos por iteraci√≥n
    - Puede ser no completo y tampoco es de soluci√≥n optima.
    - Se ocupa en redes con probas
        - Si tenemos que todos los caminos nos llevan a un camino.
    - ejem: 25
* B√∫squeda bidireccional con heur√≠stica:
    - definimos dos problemas:
        - avance: heur√≠stica normal: f_F(n)=g_F(n)+hF(n)
        - retroceso: nuestra meta es el inicio
            - acabamos cuando las dos soluciones se interceptan
    - en cada caso usamos
* py serch
* Heuristic search -Stefan Edelkamp

### Class 20 Sep
* Problema disminuidos, tener una heuristica en un problema con menos restricciones y basarnos en ella para poder crear una heuristica en el problema original(suponiendo que no la conociamos

### Assitant 27 Sep
* algoritmo greedy y A* ejemplo

> EL algoritmo m√°s corto no siempre es el menos costoso.

* El entorno es dinamico si cambia mientras el agente actua

### Class 28 Sep
* Los problemas de b√∫squeda funcionan cuando el entorno de trabajo es: totalmente observable, determinista, ep√≠sodico y discreto, estatico. 
    - estatico cuando el entorno cambia 

* Algoritmos probabilisticos:
    - Usados cuando hay incertidumbre en los problemas de IA dada por:
        - Es parcialmente observable, no hay determinismo, multiagentes que indeterminan las cirscuntancias
* l√≥gica como aproximaci√≥n para resolver los problemas
* proba:
    - proba conjunta(intersecci√≥n) = P(E1,E2) = 
    - proba condicional P(E1|E2) = 
* var aleatoria
* Agente probabilistico: Toma decisiones en base a preferencias entre posibilidades representados por evento
    - proba radial, entre m√°s te acerques mayor es la proba
        - exponencial de una metrica, en el centro es muy grande pero entre m√°s lejos no
    - Modelos basados en creencias:
        - Informaci√≥n probabilistica que el agente tiene del mundo, y puede cambiar con nuevo conocimiento
### Class 14 Oct

* Estado de creencias: Dirigidas a generar utilidad
* Utilidad: c/evento tendr√° un grado de probabilidad
    - probabilidad de la
* Teor√≠a de desici√≥n: Basado en la mayor probabilidad y utilidad
* Modelo grafico:
    - Distribuci√≥n: Proba de que dos eventos pasen
    - Estimaci√≥n de distribuci√≥n conjunta


### Class 21

- El modelo de Bayes Ingenuo es un modelo gr√°fico dirigido, representa las probabilidades conjuntas como una gr√°fica, asume que las los datos visibles x_1,...,x_n son efectos/consecuencias de la clase Y. Buscamos obtener Y, apartir de un conjunto previo de datos podremos obtener las probabilidades
    - Cada nodo estara asociado a una tabla
    - ejem 15
    - Nos permite predecir algo sin la necesidad de observar todo
    - con variables binarias podemos tener 1 como proba

- Redes bayesianas - Generalizaci√≥n de los Bayes Ingenuos: 
    - Modelos graficos dirigidos donde si tenemos una conecci√≥n entre nodos van a ser las condiciones: x->y ent x condiciona a y
    - 
### Class 24        
- Propiedad no descendiente
- Covertura de Markov
- ejemplo de red bayesiana 35
- modelos causales

### Class 26
- Investigamos un fenomeno:
    - ejemplo de red bayesiana: 40
- Predicciones de cadena

### Class 28
- Fase de entrenamiento para crear la grafica que no siempre se hace
- Ejemplo del algoritmo sobre el de markov

### Class 31
- Optimizar las creencias:
- Ejemplo

### Class 04 Nov
- Procedimiento de avance: Forward: 
    - Recorre la cadena desde el simbolo inicial hasta el final
- A = Matriz de transiciones
- B = Observables
- Procedimiento de retroceso:   Cuando veamos muchos indices podemos utilizar matrices con producto punto

### Class 07 Nov
- Algoritmo de Avance-Retroceso y Viterbi, explicaci√≥n

### Ass 08 Nov
- Backward 

### Class 09 Nov
- Ejemplo de Viterbi
    + calculamos la inicializacion, multiplicar la matriz pi por la fila de nuestro parametro de la delta
    + Producto externo(25) entre X y Y, = 
        + x1y1 * x1y2...x1yn    
        + x2y1 * x2y2...x2yn   

### Class 16 Nov

- Modelos no dirigidos:
    - En los dirigidos los factores representan la proba condicional

    - En la no dirigida los factores 
- Proba generativa: p(x|y), la var que observamos es la que esta condicionando, la y
- Discriminita : p(y|x),
- thetha son los estados de creencia, valores que representaran nuestro estado de creencia
- normalizamos para que no exceda de 1
- bias: factor de y, ajusta los datos para estar mas cerca, en el ejemplo baja los valores 
- bayes ingenuo

### Class 15,17 Nov
* Es importante recordar que en lugar de calcular una proba condicional podemos calcular una proba conjunta(y logico). 
* Un modelo gr√°fico dirigido estima la proba de una cadena y1,..,yn a partir de una cadena de entrada x1,...,xn(posibles vectores) a partir de maximizar la funci√≥n objetivo
* Observaciones: valores de X, Œ£ = {w_1 , ..., w_M },
* Simbolos de emisi√≥n: S={s 1 , ..., s N }, los valores que pueden tomar la variable Y.
* Goal: determinate a function de tal forma f:Œ£^n -> S^n de modo que una cadena la conviertan en emisiones

### Class 18 Nov
* Ejemplo de regresi√≥n logistica
* Calculamos un nuevo estado de creencias apartir de un conjunto de datos
* Evaluaci√≥n Supervisada
* Evaluaci√≥n No Supervisada: 
    - Clustering
* HOlds


### Class 25 Nov
* Descripcion del proyecto

### Class 30
- clasificar
random_state: semilla para agarrar los datos aleatoriamente
minMaxScaler los distribuye en escalas entre 0 y 1 
    - funciones de activaci√≥n: relu es util cuando no tenemos valores negativos
        - sirven para:
    - Se hace una combinaci√≥n lineal entre el peso de cada var por c/var y su respectivo sesgo
    - Eso lo recibe la funci√≥n de activaci√≥n
    - sequential
    - La primera capa tiene el n√∫mero de atributos ocupados
    - en medio
    - La ultima es el n√∫mero de resultados que queremos
    - Las capaz Dense son neuronas que se conectan
- el early_stop es una funci√≥n que se calcula la perdida, como es que el entrenamiento se aproxima a lo que ya esta

### Class 02  Dic

Las redes neuronales son varias regreciones logisticas/perceptrones

* Algoritmo k-NN:
    - analisis de varianza: las medias que minimizen la varianza
* √Årbol de decisiones: Algoritmos no parametricos
* APrendizaje no Autorizado
    - Reducci√≥n de dimensionalidad:
* k-Means

## Aprendizaje Automatico

Se dice que una m√°quina aprende de la experiencia E, si su desempe√±o con respecto a una medida P en una tarea T mejora con la experiencia E

> ¬øC√≥mo podemos construir sistemas computacionales que mejoren en una tarea a partir de la experiencia? ¬øCu√°les son las leyes fundamentales que goviernan el ‚Äúaprendizaje‚Äù?

* Podemos clasificar los modelos de aprendizaje como: 

    - Supervisado: Tenemos un conjunto supervisado S = {(x, y) : x ‚àà R d , y ‚àà Y}, tal que cada x ‚àà X se asocia a un y ‚àà Y y ≈∑ = f(x) busca ser lo m√°s cercano a cada y.
    - No-supervisado: La LM no conoce la supervisaci√≥n. f(x) es una funci√≥n de agrupamiento, asigna una clase ≈∑ que no se conoce previamente.

    Otros tipos de aprendizaje son: aprendizaje por refuerzo, aprendizaje semi-supervisado, aprendizaje auto-supervisado.

* Para llevar a cabo el entrenamiento y la evaluaci√≥n, generalmente, se dividen los datos en 3
subconjuntos:
    1. Entrenamiento: El subconjunto de datos que servir√° para estimar el modelo de aprendizaje (70%).
    
    2. Validaci√≥n: Es un subconjunto de datos que se prueba constantemenete para ajustar ciertos requerimiento del modelo de aprendizaje (10-15%).

    3. Evaluaci√≥n: Es el subconjunto a partir del cu√°l se determina la capacidad de generalizar del modelo estimado (15-20%).

https://www.kaggle.com/code/alb3rtein/naive-base-classifier para naive

https://github.com/crazzylearners/Machine-Learning/blob/master/K%20Means%20Clustering%20(sklearn).ipynb
# Lab

El mejor punto es el que menos clasificaciones tienes erroneas


# ask about import
- Ejemplo de Bayes Naive, para determinar la clase a partir de:
- En una grafica bayesiana, no entiendo a que se refieren con consultas-sacar la proba de ese estado, solo hay que multiplicar la proba condicional por sus padres de cada nodo 
- Modelo Oculto de Markov de una graf
- Usarlo para calcularlo la proba
- Algoritmo de Viterbi
- 6 Modelo de bayes ingenuo: proba de secuencias?
- demostrar what
- wow

# Lab

* Qu√© significa para ti √©xito? De acuerdo a eso debes de checar como ser√°n tus recompensas. Procurar la recompensa acumulada[generada?]

* Poliza: 
    - Entrenar, nos quedamos con los mejores agentes, usas su historia para obtener una p√≥liza, una vez hecho volvemos a entrenar pero con la poliza basada 
    - Para seguir explorarando pero teniendo una poliza suboptima.
    - Variables primitivas *X*, variable que quieres predecir *Y*.
* Entrenar, recopilar datos, quedarnos con los mejores datos y luego pasarle un modelo de machine learning.

* Cuando tienes problemas de clasificaci√≥n
    - Presici√≥n: De todas las personas que la prieba dijo que ten√≠an covid, ¬øC√∫antas realmente lo tenian?
    - De todas las personas que tenian covid realmente cuantas el modelo identifico.
        Cuando tenemos muchas sensibilidades tambi√©n cometeras herrores de personas que tu dijiste que ten√≠as pero no ten√≠an. 

* Las recompensas inmediata no siempre suele ser lo mejor.

* reshape(-1,1) nos permite 
* en pandas siempre empezamos por los rows, luego las columnas
    - La desviacion estandar nos dice como se distribuyen los datos
- Sabiendo que distribuci√≥n sigue podremos calcular las probas

#### Important
- primero debemos de saber como son nuestras variables

------
## Expressions of gratitude üéÅ
* :punch: Share and tell others about this notes üì¢
* :+1: Contact and follow me :bowtie:
------
‚å®Ô∏è with much :purple_heart: by [Jose-MPM](https://github.com/Jose-MPM) üòä‚å®Ô∏è