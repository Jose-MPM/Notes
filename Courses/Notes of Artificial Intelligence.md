# Artificial Intelligence
## Profesor: VÃ­ctor GermÃ¡n Mijangos de la Cruz

El campo de la Inteligencia Artificial (IA) se enfoca tanto en el estudio como en la
construcciÃ³n de entidades inteligentes. 

Podemos plantearnos las siguientes preguntas:
- Â¿QuÃ© tipo de entidades nos interesan?
- Â¿QuÃ© implica que una entidad sea inteligente?

Podemos distinguir los siguientes Ã¡mbitos:
- Considerando si se refiere al humano:
    1. Inteligencia en tanto fidelidad al actuar humano: 
        - Pensamiento del humano: Podemos decir que una entidad es inteligente si sabemos su programaciÃ³n es similar a la forma en que pensamos los humanos. Las ciencias cognitivas buscan construir teorÃ­as del cerebro humano.
        
        Algunos pasos que se pueden seguir son:
        - IntrospeccYa queiÃ³n
        - ExperimentaciÃ³n psicolÃ³gica
        - ImÃ¡genes cerebrales

    2. Inteligencia en tanto â€œllegar a un buen finâ€ o racionalidad:
        - Pensamiento racional: Una entidad es inteligente si su pensamiento es el adecuado para llegar a la soluciÃ³n de un problema. Necesitamos de un pensamiento lÃ³gico que nos lleve a tomar las decisiones adecuadas.

- Considerando el punto de referencia:
    1. Inteligencia como atributo interno, como procesos de pensamiento y razonamiento.
    2. Inteligencia como atributo externo, comportamiento.

## Day 2
### Enfoques:

        -  | Interno | Exterior|
____ 
    Humano | CogniciÃ³n | Racional|
___
    Racional| 
___
* Comportamiento Racional:
    - Agente: Algo que actua, a partir de su ambiente percibido y adaptandose.
        - Agente racional: Actuar de una forma Ã³ptima para obtener el mejor resultado esperado.
* Modelo EstÃ¡ndar: Agentes racionales que solucionen tareas de manera adecuada,
    - Las inferencias correctas son parte de sus mecanismos.
    - Adaptable a perspectivas cientÃ­ficas, usa formalismos matemÃ¡ticos para representarlos.

* History of the AI:
    - Maquinas pensantes: Resolver problemas a partir de la lÃ³gica. _La inteligencia se asocia al pensamiento lÃ³gico_.

    - Tesis de Turing:
        > Para culquier sistema forma determinista, existe una MT formalmente equivalente.
    - **Bar-Hillel**:
        > Cualquier sistema capaz de hacer una traducciÃ³n autÃ©ntica y precisa debe saber lo que sabe la gente y debe aplicar ese conocimiento de forma sensata. 
    - MÃ©todos dÃ©biles: mecanismos con prÃ³posito general que usan pasos de razonamiento elemental para encontrar soluciones completas.[dominio especÃ­fico]

    - Micromundos: Donde interactua nuestro agente, un entorno delimitado[dominio especÃ­fico].
    - Sistemas expertos: limitar la actuaciÃ³n de la mÃ¡quina a un dominio especÃ­fico buscando que se comporte como un humano experto.
    - En los Ãºltimos aÃ±os todo estÃ¡ en redes neuronales[Deep Learning]

## Monday August 22

* Agente: Entidad que tiene sensores que le ayudan a percibir el ambiente a traves de sensores
    
    - Secuencia de percepciones = historia completa de lo percibido por el agente hasta el momento actual.

    - Una **acciÃ³n** depende del conocimiento que tenemos a partir de su secuencia de percepciones, *actuar* a partir de percepciones pasadas[experiencia].
        - Entre mÃ¡s experiencia podremos tomar una mejor decisiÃ³n

    - **FunciÃ³n de agente:** Mapeo utilizado para pasar de la cadena de percepciones hacia las acciones. La funciÃ³n mapea las percepciones de las acciones:
        - input: secuencia de persepciones
        - output: acciÃ³n
    Esta determinara el comportamiento de nuestro agente. (AÃºn tenemos limitaciones, por el determinismo)

* Racionalidad y agentes racionales:
    - Agente que realiza acciones Ã³ptimas con base en su experiencia.
        + Ã“ptimo: El consecualismo seÃ±ala que podemos evaluar el comportamiento de un agente a partir de sus consecuencias necesitando asÃ­ poder medir el rendimiento:
            * Es decir, necesitamos medir nuestro rendimiento[es decir, nuestras consecuencias]
                - > Regla general: _es mejor medir la utilidad de acuerdo con lo que se quiere en el entorno, mÃ¡s de acuerdo con cÃ³mo se cree que el agente debe comportarse.
                - Medida de rendimiento:

    - Racionalidad: 
        1. Medida de rendimiento que define el criterio de Ã©xito.
        2. Conocimiento previo acerca del ambiente[entorno].
        3. Acciones que se pueden realizar
        4. Secuencia de percepciones[experiencia, memoria]
    Buscando una mejora con el tiempo. 
    
    - Agente racional: Agente que selecciona la _acciÃ³n_ que maximice la _medida de rendimiento_ a partir de la _experiencia_(secuencia de percepciones) y su _conocimiento previo_ a cerca del ambiente.

    - Agente omniciente: Agente que conoce  el resultado de sus acciones y puede actuar de acuerdo a esto.

    - Tipos de agente:
        1. Agente activo: Todo depende del estado actual, ya que es lo Ãºnico que ve: _el entorno actual_ y a partir de ahÃ­ decide como actÃºa.
        2. Adente dividido por tabla: Es capaz de usar memoria

    - Racionalidad busca maximizar el rendimiento **esperado**
    - La perfecciÃ³n maximiza el redimiento **real**.
        + Racionalidad != Omniciencia[Dios]

## Assitant Tuesday August 23

- IA: Simular el comportamiento humano. Acciones aÃºtonomas aunque no aprenda.
- ML: La maquina aprende, a traves de la experiencia.
- DL: Aprendizaje utilizando redes neuronales.

Un agente dentro de un determinado entorno necesita de sensores para percibir las persepciones de su entorno y de un actuador[capacidad del agente para definir x acciÃ³n] que nos permite hacer acciones en nuestro entorno[modificandolo], el agente tiene una metrica de evaluaciÃ³n y a partir de esa evaluaciÃ³n tenemos una respuesta[acciÃ³n] (funciÃ³n de transiciÃ³n)

## Class Wednesday August 24

* Naturaleza de los ambientes:
    - Entorno de trabajo: es especificado por la medida de rendimiento, el ambiente, asÃ­ como los actuadores y los sensores del agente.
    
    - En el diseÃ±o del agente implica diseÃ±ar los elementos del entorno de trabajo:
        + Medidas de rendimiento
        + Entorno
        + Actuadores
        + Sensores
    
    - Entornos observables:
        + Entorno completamente observable:
            * El entorno es asÃ­ si los sensores del agente tiene acceso a la informaciÃ³n total de un estado del ambiente en x tiempo.
            * Toda la informaciÃ³n **de todo el entorno** en un estado del ambiente
        + Entorno parcialmente observable: 
            + informaciÃ³n parcial del ambiente
                * Manejo automatico de autos
        + Efectivamente totalmente observable:
            - **Todo desde la perspectiva del agente-**
            - Aparte de toda la info, tenemos la suficiente informaciÃ³n para saber que acciÃ³n realizar **la adecuada**.
            -  Si el agente detecta los aspectos relevantes para realizar una acciÃ³n.
            (Si supieramos como se mueven los fantasmas podrÃ­amos decir que siempre nos movemos efectivamente.)
        + No obsevable: Si el agente no tiene sensores, el entorno es no observable.
    
    - El entorno determina si hablamos de un agente individual o multiagentes, depende de las acciones que quieramos realizar.
        + Individual: Un solo agente.
        + Multiagente: Dos o mÃ¡s agentes que se afecten entre sÃ­.
            1. Competitivo: Maximizar el rendimiento de un agente implica que se minimiza el rendimiento de otros agentes.
                - Ajedrez
            2. Cooperativo: Las acciones de cada agente se orientan a maximizar el rendimiento de todos los agentes
                - Auto sin conductor
    - Entorno Determinista:
        - El estado estÃ¡ determinado completamente por las acciones del agente y de su estado previo(Si existen aspectos no observables, entonces es preferible tratar el entorno como estocÃ¡stico)
        - Entorno EstocÃ¡stico(No-Determinista):
            // Una acciÃ³n puede llevar a mÃ¡s de un resultado
            - El estado puede cambiar no necesariamente por una acciÃ³n del agente.
        - Entorno EstratÃ©gico:
            - Las acciones son deterministas pero siempre habrÃ¡n resultados de otros agentes. Hay acciones de otros agentes que pueden modificar el edo del entorno.
            El entorno puede cambiar a pesar de mi si hay otros agentes.
    - Secuencia:
        - Entorno Episodico:
            - Episodios independecias entre si, entre que va a pasar en un episodio y otro, en cada uno de los cuales el agente reciba una persepciÃ³n y ejecuta una acciÃ³n en base en Ã©sta.
                * Las acciones son independientes entre si. No hay correlaciÃ³n entre sÃ­.
        - Entorno secuenciales:
            - La desiciÃ³n actual puede afectar a las futuras decisiones del agente, acciones influyentes entre si.
    - Dinamica **=!** 
        - Entorno estÃ¡tico: No cambia cuando el agente delibera
        - Entorno dinÃ¡mico: Cambia independientemente del agente
        - SemidinÃ¡mico: Cambia nuestra capacidad de reaccionar del agente y cambia el entorno
    - Discreto: estados, percepciones, y acciones finitas 
    - Continuo: Entorno cambiante, cuando es dificil definir estados entre acciones, cuando no es posible enumerar los estados no se si estÃ¡ bi
    [Es continuo porque siempre tenemos una desiciÃ³n diferente.]
        - Conocido: resultados de las acciones conocidos de antemano. [Se sabe el accionar],
        - Desconocido: No hay certeza de como se comporta el entorno, no se conocen los resultados de las acciones, cuando el agente no sabe como va a acabar.
            + DetecciÃ³n de spam.
    - En el modelo de lo que quieres, todo es a su conveniencia
    

## Class Monday 29 August

* El problema de un agente dirigido mediante tabla es el nÃºmero de bÃºsquedas que debe hacer en la tabla; Entorno: Completamente observable, determinista, estÃ¡tico, discreto. Tiene memorÃ­a peroe s

* _Agente reactivo simple_. Ignora la historia de percepciones, los agentes contienen reglas de _condiciÃ³n-acciÃ³n_. Entorno episodico y no guarda nada, ninguna percepciÃ³n, solo devuelve el estado

* _Agente reactivo basado en modelos._ es un agente que se basa en un modelo del mundo determinado por nosotros
    - Modelo de transiciÃ³n, actuadores: modelar las acciones/decisiones
    - Modelo de sensor: Como percibe el agente al mundo, a partir de los sensores vemos como es el mundo en ese momento.

* _Agente basado en objetivo_: Cuenta con informaciÃ³n acerca de un objetivo. 

* _Agente basado en utilidad_: indica cÃºales son preferibles al comparar los resultados, busca maximizar el valor esperado de esta funciÃ³n.

* Agente por aprendizaje: _Que aprende_: Se aprende cuando una agente que opera sobre un entorno desconocido mejora su rendimiento a partir de observar resultados previos de sus acciones y evaluarlos.

## Class Wednesday 31 August

- Utilitarismo

- Agentes para resolver problemas:
    + Agente basado en objetivo que realiza una planeaciÃ³n para cumplir con los objetivos dados.
    + Para solucionar los problemas el agente se basan en la _bÃºsqueda_.
        1. Busqueda Informada: El agente puede estimar que tan lejos se encuentra del objetivo.
        2. BUsqueda Desinformada: No es capaz de estimar su distancia del objetivo, no tenemos informaciÃ³n precisa relacionada con el objetivo.
            
            > Si estamos fallando en planear, estamos planeando fallar.

- Para resolver el problema necesitamos definir(la planeaciÃ³n):
    - FormulaciÃ³n del objetivo: Determinar estado objetivos.
    - FormulaciÃ³n del problema: DescripciÃ³n de estados y las acciones para alcanzar un objetivo(abstracciÃ³n del mundo).
    - BÃºsqueda: Simula la secuencia de acciones que lleven al objetivo, elegiendo la mÃ¡s Ã³ptima de acuerdo a una funciÃ³n que decidamos.
    - EjecuciÃ³n: 

- About example:
    * Las acciones mueven nuestro agente entre los estados.
        + Y las acciones del agente por edo tambiÃ©n? 
- Sistema de ciclos:
    1. Sistema de ciclos abiertos(Open loop): Una vez que el agente ha encotrado la soluciÃ³n, la ejecuta ignorando las persepciones mientras lo hace.[No deja de percibir, solo las ignora.].
    2. Sistema de ciclos cerrado(Closed loop): Son aquellas en el que el agente ejecuta la soluciÃ³n, pero mientras lo hace moritorea las percepciones que recibe. [Podemos incorporarlas].

- Problema de bÃºsqueda formalizado: 6-tupla = AÃºtomata.
    + S=s1...sn, conjunto de estados
    + S_o in S, edo inicial
    + F subconjunto S, edos finales
    + A = a1...an, conjunto de acciones
    + T: SxA -> S, modelo de transiciÃ³n.
    + c: funciÃ³n de costo

- Soluciones:
    + Camino: Secuencia de acciones.
    + SoluciÃ³n: Camino que te lleva a la soluciÃ³n, i.e. termina en un objetivo.
    + SoluciÃ³n Ã³ptima: SoluciÃ³n que minimiza la funciÃ³n de costo sobre el camino. 

### Class Thursday September 1

* _Agente reactivo basado en modelos._ es un agente que se basa en un modelo del mundo determinado por nosotros
    - Modelo de transiciÃ³n, actuadores: modelar las acciones/decisiones
    - Modelo de sensor: Como percibe el agente al mundo, a partir de los sensores vemos como es el mundo en ese momento.

El entorno va cambiando el entorno ddl agente.

* Agente basado en objetivos: 
Estado --> CÃ³mo es el mundo ahora? **-->** Estado
Dentro de sus actuadores puede cambiar el estado 

### Class Friday September 2
* No buscamos el mÃ­nimo, si no el argumento que mÃ­nimiza la funciÃ³n. 
    - arg min  $x^2 + 1=1

* Algoritmo de bÃºsqueda:
    - Problema de bÃºsqueda -> Alg de bÃºsqueda -> SoluciÃ³n

* Ãrbol de bÃºsqueda: describe los caminos entre un estado inicial y el objetivo.
    - Edo inicial = raiz
    - Edo final = hoja
    - ExpanciÃ³n de Nodo: teniendo un nodo se dice expansiÃ³n del nodo si podemos cambiar de estado a travez de la funciÃ³n de transciciÃ³n.
    - Frontera: estados dados que no se ha expandido.
        + Siempre tenemos que revisar la frontera para ver que podemos hacer, ver cual nos conviene seguir expandir, "elegir estrategia"
        + La frontera  es una pila que guarda informaciÃ³n de los nodos obtenidos.
            - Pila de prioridad: El tope de la pila es el elem con menor valor de acuerdo a una funciÃ³n de costo
            - FIFO(First-in-First Out) EL tope de la cola es el primero
            - LIFO(Last-in-First Out) EL Ãºltimo en entrar en la cola es el primero
            - Operaciones:
                - pop = expandir nuestro nodo
    - Algoritmo Best-First Search, busca encontrar el camino mÃ¡s adecuado.
        - Ocupa hacer pop al estado menos costoso y expandir ese nodo.
        - FIFO

### Class 07 Sep

* Busqueda desinformada
    - si no contamos con informaciÃ³n de la cercanÃ­a de este estado con la meta
    - para revisar si un nodo es una meta:
        - Early Goal Test: checar si el nodo es una meta antes de expandir
        - Late Goal Test: checamos el nodo despues de eliminaf de la pila|


### Class 09 Sep
* busqueda desinformada
    EARLY: revisa antes de expandir
    Late: despues

* Breadth-FIst: algoritmo de busca desinformada, expande todos sus hijos, no toma la funciÃ³n de costo,frontera: pila FIFO, el primero que entre es el que sale, en un nodo de profundidad d, se han revisado los d-1 niveles.

* Dijkstra: Pila de prioridad dado por la funciÃ³n del costo, no podemos usarlo si no tenemos una funciÃ³n de costo, porque sino serÃ­a un fuerza bruta

*  Depth-First: No toma en cuenta el costo, expande primero el nodo de mayor prioridad
    - implementacion tipo Ã¡rbol, no guarda una tabla de los nodos alcanzados
    - Best-First Search con funciÃ³n negativa(LIFO)
    - devuelve la primera soluciÃ³n que encuentra, no la Ã³ptima
    - solo es completo cuando tengamos una estructura de Ã¡rbol, en ciclos muere, no podemos garantizar que converga.
    - Ejemplo: 15 A
        - no guarda lo que vamos alcanzando
        - abre todos los hijos de un lado

* Depth-Limited Search: es  un tipo Depth First limitando la profundidad mayor a cierta profundidad
    - (25) fuerza bruta limitada, tope de hasta donde vas a expandir
    - 30 code
    - Problema: no encontrar una soluciÃ³n cuando se encuentra mÃ¡s alla de la profundidad escogida.
        - la soluciÃ³n estÃ¡ en encontrar el diametro, pero eso puede ser imposible calcularlo

* Iterative Deepening Search[40] (mejor versiÃ³n de profundidad)
    - Corta en la soluciÃ³n exacta, ya que no necesita explorar profundidades de mÃ¡s
    - Completo: porque podra encontrar la soluciÃ³n aunque se tarde mucho

### Class 12 Sep
* Bidirectional Search: Buscar de manera simultÃ¡nea en forma de avance(desde el inicio al final) y de forma de retroceso.
    + Ejemplo:25
+ Algoritmos de bÃºsqueda desinformada.
    - 
+ Busqueda informada (heurÃ­stica): se basa en la integraciÃ³n de una funciÃ³n heurÃ­stica:
    - FunciÃ³n heurÃ­stica: 


    + Greedy Best-First Search:
        - expande en el primer lugar el nodo n con el valor h(n) mÃ¡s pequeÃ±o, es decir: f(n)=h(n).
        - ambicioso por llegar rÃ¡pido sin tomar en cuenta los costos.
    + Busqueda A* : expande el nodo, n que minimiza la funciÃ³n f(n)=g(n)+h(n) donde g es el costo desde el edo init al nodo n, y h es una funciÃ³n heurÃ­stica al costo del nodo n hacia un edo f.

    Completo: podemos garantizar que podemos llegar del edo inicial al final sin bucles.
    
### Assistant 13 Sep
algorithm DF: lo mÃ¡s profundo hasta encontrar la solucion
note of A*, plantilla, get_path*
En el 4 las acciones las puedes definir tu [arriba y abajo]

# Lab

* QuÃ© significa para ti Ã©xito? De acuerdo a eso debes de checar como serÃ¡n tus recompensas. Procurar la recompensa acumulada[generada?]

* Poliza: 
    - Entrenar, nos quedamos con los mejores agentes, usas su historia para obtener una pÃ³liza, una vez hecho volvemos a entrenar pero con la poliza basada 
    - Para seguir explorarando pero teniendo una poliza suboptima.
    - Variables primitivas *X*, variable que quieres predecir *Y*.
* Entrenar, recopilar datos, quedarnos con los mejores datos y luego pasarle un modelo de machine learning.

* Cuando tienes problemas de clasificaciÃ³n
    - PresiciÃ³n: De todas las personas que la prieba dijo que tenÃ­an covid, Â¿CÃºantas realmente lo tenian?
    - De todas las personas que tenian covid realmente cuantas el modelo identifico.
        Cuando tenemos muchas sensibilidades tambiÃ©n cometeras herrores de personas que tu dijiste que tenÃ­as pero no tenÃ­an. 

* Las recompensas inmediata no siempre suele ser lo mejor.

### aumento de calidad de video
Para aumentar la calidad del video debemos aumentar el nÃºmero de pixeles, cada video esta dado por imagenes

Aumentar la resoluciÃ³n: 700p -> 1080p

- Observable:
- Agente:
- Determinista:
- EpisÃ³dico:
- EstÃ¡tico:
- Discreto: 

------
## Expressions of gratitude ğŸ
* :punch: Share and tell others about this notes ğŸ“¢
* :+1: Contact and follow me :bowtie:
------
âŒ¨ï¸ with much :purple_heart: by [Jose-MPM](https://github.com/Jose-MPM) ğŸ˜ŠâŒ¨ï¸